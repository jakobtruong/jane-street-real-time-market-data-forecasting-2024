{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84493,"databundleVersionId":9871156,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, parquet file I/O (e.g. pd.read_parquet)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearnex import patch_sklearn\npatch_sklearn()\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T06:44:18.702368Z","iopub.execute_input":"2024-11-27T06:44:18.702930Z","iopub.status.idle":"2024-11-27T06:44:21.151040Z","shell.execute_reply.started":"2024-11-27T06:44:18.702857Z","shell.execute_reply":"2024-11-27T06:44:21.149448Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/jane-street-real-time-market-data-forecasting/responders.csv\n/kaggle/input/jane-street-real-time-market-data-forecasting/sample_submission.csv\n/kaggle/input/jane-street-real-time-market-data-forecasting/features.csv\n/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=4/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=5/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=6/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=3/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=1/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=8/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=2/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=0/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=7/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=9/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/lags.parquet/date_id=0/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet/date_id=0/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/kaggle_evaluation/jane_street_gateway.py\n/kaggle/input/jane-street-real-time-market-data-forecasting/kaggle_evaluation/jane_street_inference_server.py\n/kaggle/input/jane-street-real-time-market-data-forecasting/kaggle_evaluation/__init__.py\n/kaggle/input/jane-street-real-time-market-data-forecasting/kaggle_evaluation/core/templates.py\n/kaggle/input/jane-street-real-time-market-data-forecasting/kaggle_evaluation/core/base_gateway.py\n/kaggle/input/jane-street-real-time-market-data-forecasting/kaggle_evaluation/core/relay.py\n/kaggle/input/jane-street-real-time-market-data-forecasting/kaggle_evaluation/core/kaggle_evaluation.proto\n/kaggle/input/jane-street-real-time-market-data-forecasting/kaggle_evaluation/core/__init__.py\n/kaggle/input/jane-street-real-time-market-data-forecasting/kaggle_evaluation/core/generated/kaggle_evaluation_pb2.py\n/kaggle/input/jane-street-real-time-market-data-forecasting/kaggle_evaluation/core/generated/kaggle_evaluation_pb2_grpc.py\n/kaggle/input/jane-street-real-time-market-data-forecasting/kaggle_evaluation/core/generated/__init__.py\n","output_type":"stream"},{"name":"stderr","text":"Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Loading Dataset\nGiven the large dataset and kaggle kernel memory limitations, the data will be loaded into the dataframe in chunks. In addition, I will be using a memory reducing function to type cast all the float and int data types in our dataset to their space efficient data type without affecting their values. (Precision of floating point values may be affected but this downside will not affect training by much)","metadata":{}},{"cell_type":"code","source":"def reduce_memory_usage(df, float16_as32=True):\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n        if col_type != object and str(col_type)!='category':\n            c_min, c_max = df[col].min(), df[col].max()\n\n            # Reduces all int datatypes in dataframe to smallest datatype possible given the column's min/max values\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n\n            # Reduces all float datatypes in dataframe to smallest datatype possible given the column's min/max values\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    if float16_as32:\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        df[col] = df[col].astype(np.float16)  \n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        \n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f}MB'.format(end_mem))\n    print('Decreased by {:.1f}% \\n'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T06:44:21.153363Z","iopub.execute_input":"2024-11-27T06:44:21.153966Z","iopub.status.idle":"2024-11-27T06:44:21.168634Z","shell.execute_reply.started":"2024-11-27T06:44:21.153925Z","shell.execute_reply":"2024-11-27T06:44:21.166830Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Initialize a list to hold samples from each file\nsamples = []\n\n# Load a sample from each file\nfor i in range(10):\n    file_path = f\"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id={i}/part-0.parquet\"\n    print('Processing file: ', file_path)\n    chunk = pd.read_parquet(file_path)\n    reduce_memory_usage(chunk, False)\n    \n    # Take a sample of the data (adjust sample size as needed)\n    # chunk_sample = chunk.sample(n=50000, random_state=14)\n    samples.append(chunk)\n    \n# Concatenate all samples into one DataFrame if needed\ndf = pd.concat(samples, ignore_index=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T06:44:21.170349Z","iopub.execute_input":"2024-11-27T06:44:21.170776Z"}},"outputs":[{"name":"stdout","text":"Processing file:  /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=0/part-0.parquet\nMemory usage of dataframe is 654.51 MB\nMemory usage after optimization is: 435.72MB\nDecreased by 33.4% \n\nProcessing file:  /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=1/part-0.parquet\nMemory usage of dataframe is 944.04 MB\nMemory usage after optimization is: 548.24MB\nDecreased by 41.9% \n\nProcessing file:  /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=2/part-0.parquet\nMemory usage of dataframe is 1022.35 MB\nMemory usage after optimization is: 593.72MB\nDecreased by 41.9% \n\nProcessing file:  /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=3/part-0.parquet\nMemory usage of dataframe is 1352.24 MB\nMemory usage after optimization is: 693.36MB\nDecreased by 48.7% \n\nProcessing file:  /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=4/part-0.parquet\nMemory usage of dataframe is 1690.96 MB\nMemory usage after optimization is: 867.04MB\nDecreased by 48.7% \n\nProcessing file:  /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=5/part-0.parquet\nMemory usage of dataframe is 1800.46 MB\nMemory usage after optimization is: 923.18MB\nDecreased by 48.7% \n\nProcessing file:  /kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=6/part-0.parquet\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"pd.set_option('display.max_columns', None) # Sets an option to let pandas show all columns. Without this, the columns will be truncated.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df[df.columns.drop(list(df.filter(regex='responder_[^6]')))]\ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print the number of rows\nprint(f\"Total number of rows: {len(df)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count the unique 'date_id' values and the number of years these dates add up to\nunique_dates = df['date_id'].nunique()\n\nprint(f\"Number of unique days (date_id): {unique_dates}\")\nprint(f\"Number of years: {unique_dates/365.25:.2f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.imshow(df, cmap='hot', interpolation='nearest')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}